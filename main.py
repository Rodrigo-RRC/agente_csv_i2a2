# main.py

# 1. ImportaÃ§Ãµes para o agente de linguagem e variÃ¡veis de ambiente

from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from dotenv import load_dotenv
import os


# 2. ImportaÃ§Ãµes dos mÃ³dulos internos do projeto

from agente_inteligente.ler_csv import ler_csvs_notas
from agente_inteligente.filtros import filtrar_por_estado, filtrar_por_municipio


# 3. Carrega a chave da OpenAI do arquivo .env

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 4. LÃª os arquivos CSV (cabecalho e itens)

df_cabecalho, df_itens = ler_csvs_notas()



# 5. Apresenta uma prÃ©via dos dados

print("\nðŸ“„ Amostra do cabeÃ§alho original:")
print(df_cabecalho.head())
print("\nðŸ“Œ Atributos disponÃ­veis no DataFrame:")
for coluna in df_cabecalho.columns:
    print(" â””â”€", coluna)


# 6. Aplica filtro por estado (exemplo: ParaÃ­ba - "PB")

uf_escolhida = "PB"
df_filtrado = filtrar_por_estado(df_cabecalho, uf_escolhida)

municipio_escolhido = "JOÃƒO PESSOA"
df_filtrado = filtrar_por_municipio(df_filtrado, municipio_escolhido)



# 7. Gera pergunta com base nos dados filtrados

quantidade = len(df_filtrado)
mensagem_usuario = (
    f"Foram encontradas {quantidade} notas fiscais emitidas no municÃ­pio de {municipio_escolhido} ({uf_escolhida}). "
    "Qual a importÃ¢ncia de analisar essas emissÃµes por localidade?"
)


# 8. Inicializa o modelo da OpenAI

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key=api_key,
    temperature=0.3
)


# 9. Envia a pergunta e recebe a resposta
# ðŸ”§ Prompt mÃ­nimo para testes controlados (evita uso aleatÃ³rio de tokens)

mensagem_usuario = "Responda exatamente com a palavra: Teste"
resposta = llm.invoke([HumanMessage(content=mensagem_usuario)])



# 10. Exibe a resposta do agente

print("\nðŸ’¬ Resposta do agente:")
print(resposta.content)
